{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plot_param_space",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1txjaHTeJwEAHjoQFusJ5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPZ39tHdBO5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEwfkNbvowzs"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import glob, os\n",
        "\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLRJ7pjTpHl0"
      },
      "source": [
        "INSTRUMENT = 'eva' \n",
        "DRIVE_BASE_DIR = '/content/drive/My Drive/SMC 09/DDSP/eval'\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "legend=[]\n",
        "\n",
        "for hyperparams in [(20,10),(20,35),(20,65),(60,10),(60,35),(60,65),(100,10),(100,35),(100,65)]:\n",
        "\n",
        "    harmonic_distribution, noise_magnitudes = hyperparams  \n",
        "    LOG_DIR = DRIVE_BASE_DIR + '/' + INSTRUMENT + \\\n",
        "                '_h' + str(harmonic_distribution) + \\\n",
        "                '_n' + str(noise_magnitudes) + \\\n",
        "                '_checkpoints' + \\\n",
        "                '/summaries/train/'\n",
        "    print(\"Analyzing\", LOG_DIR)\n",
        "    legend.append('h: {}, n:{}'.format(harmonic_distribution, noise_magnitudes))\n",
        "\n",
        "    os.chdir(LOG_DIR)\n",
        "    logs=[]\n",
        "    for file in glob.glob(\"*.v2\"):\n",
        "        logs.append(file)\n",
        "\n",
        "    losses = {}\n",
        "    for log_file in logs:\n",
        "        for e in tf.compat.v1.train.summary_iterator(LOG_DIR+log_file):\n",
        "            for v in e.summary.value:\n",
        "                if v.tag == 'losses/total_loss':\n",
        "                    tensor_array = tf.io.decode_raw(v.tensor.tensor_content, v.tensor.dtype)\n",
        "                    losses[e.step] = tensor_array[0]\n",
        "                    if e.step==20000:\n",
        "                      print(tensor_array[0])\n",
        "    ax.plot(list(losses.keys()), list(losses.values()))\n",
        "\n",
        "ax.set_ylabel('total loss')\n",
        "ax.set_xlabel('steps')\n",
        "ax.legend(legend)\n",
        "ax.grid(b=True)\n",
        "_ = ax.set_ylim((5,7.5))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}