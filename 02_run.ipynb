{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02_run",
      "provenance": [],
      "collapsed_sections": [
        "cDjCZ1hHEHmS",
        "BMH14x2OFA0c"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkCBYJgyvGAc"
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDjCZ1hHEHmS"
      },
      "source": [
        "#Install, import and setup files\n",
        "1.   Install DDSP\n",
        "2.   Import Python libraries\n",
        "3.   Configure folders\n",
        "4.   Copy instruments and examples to Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhqoaVStEDaj"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install -qU ddsp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMZI6KnSEjAG"
      },
      "source": [
        "import warnings\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import gin\n",
        "import librosa\n",
        "import pickle\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from ddsp.colab import colab_utils\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, audio_bytes_to_np,\n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "from google.colab import files\n",
        "from ipywidgets import interact\n",
        "\n",
        "\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "#Use retina mode\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "#Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Set some globals\n",
        "REALEXAMPLE = ''\n",
        "REALINSTRUMENT = ''\n",
        "audio = None\n",
        "audio_features = None\n",
        "ckpt = None\n",
        "model = None\n",
        "DATASET_STATS = None\n",
        "\n",
        "TRIM = -15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9fQSNVrwreY"
      },
      "source": [
        "DRIVE_DIR = '/content/drive/My Drive/SMC 09/DDSP' #@param {type: \"string\"}\n",
        "INSTRUMENTS_FOLDER = 'instruments'\n",
        "EXAMPLES_FOLDER = 'examples'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7MO01A1_JMY"
      },
      "source": [
        "EXAMPLES_DIR = DRIVE_DIR + '/' + EXAMPLES_FOLDER\n",
        "assert os.path.exists(EXAMPLES_DIR)\n",
        "COLAB_EXAMPLES_DIR = 'data/examples' \n",
        "!mkdir -p $COLAB_EXAMPLES_DIR\n",
        "\n",
        "mp3_files = glob.glob(os.path.join(EXAMPLES_DIR, '*.mp3'))\n",
        "wav_files = glob.glob(os.path.join(EXAMPLES_DIR, '*.wav'))\n",
        "audio_files = mp3_files + wav_files\n",
        "example_file_list = [os.path.basename(file) for file in (audio_files)]\n",
        "example_file_list.sort()\n",
        "\n",
        "print(\"Copying {} examples\".format(len(example_file_list)))\n",
        "for fname in audio_files:\n",
        "    target_name = os.path.join(COLAB_EXAMPLES_DIR, \n",
        "                               os.path.basename(fname).replace(' ', '_').replace('\\'', '_'))\n",
        "    #print('  {}'.format(target_name))\n",
        "    !cp \"$fname\" $target_name\n",
        "\n",
        "\n",
        "INSTRUMENTS_DIR = DRIVE_DIR + '/' + INSTRUMENTS_FOLDER\n",
        "assert os.path.exists(INSTRUMENTS_DIR)\n",
        "COLAB_INSTRUMENTS_DIR = 'data/instruments' \n",
        "!mkdir -p $COLAB_INSTRUMENTS_DIR\n",
        "\n",
        "zip_files = glob.glob(os.path.join(INSTRUMENTS_DIR, '*.zip'))\n",
        "instrument_file_list = [os.path.basename(file) for file in (zip_files)]\n",
        "instrument_file_list.sort()\n",
        "\n",
        "print(\"Copying {} instruments\".format(len(instrument_file_list)))\n",
        "for fname in zip_files:\n",
        "    target_name = os.path.join(COLAB_INSTRUMENTS_DIR, \n",
        "                               os.path.basename(fname).replace(' ', '_').replace('\\'', '_'))\n",
        "    #print('  {}'.format(target_name))\n",
        "    !cp \"$fname\" $target_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMH14x2OFA0c"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zHRDn7fFElh"
      },
      "source": [
        "def load_example(example_name):\n",
        "\n",
        "  global REALEXAMPLE, audio, audio_features\n",
        "\n",
        "  if (REALEXAMPLE!=example_name):\n",
        "    print('Loading audio and extracting features...')\n",
        "    REALEXAMPLE=example_name\n",
        "    start_time = time.time()\n",
        "\n",
        "    with open(COLAB_EXAMPLES_DIR+'/'+REALEXAMPLE, 'rb') as fd:\n",
        "      contents = fd.read()\n",
        "\n",
        "    audio = audio_bytes_to_np(contents, sample_rate=DEFAULT_SAMPLE_RATE, normalize_db=None)\n",
        "    audio = audio[np.newaxis, :]\n",
        "\n",
        "    #Compute features.\n",
        "    ddsp.spectral_ops.reset_crepe()\n",
        "    audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "    audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "    print('Operation took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "    #fig, ax = plt.subplots(nrows=3, \n",
        "    #                   ncols=1, \n",
        "    #                   sharex=True,\n",
        "    #                   figsize=(12, 8))\n",
        "    #ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
        "    #ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "    #ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "    #ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "    #ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
        "    #ax[2].set_ylabel('f0 confidence')\n",
        "    #_ = ax[2].set_xlabel('Time step [frame]')\n",
        "\n",
        "    if REALEXAMPLE != '' and REALINSTRUMENT != '':\n",
        "      print(\"Restoring model from load_example\")\n",
        "      restore_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCpZbpQZP8Ry"
      },
      "source": [
        "def load_instrument(instrument_name):\n",
        "\n",
        "  global REALINSTRUMENT, model, ckpt, DATASET_STATS\n",
        "\n",
        "  if (REALINSTRUMENT!=instrument_name):\n",
        "    print('Loading the model...')\n",
        "    REALINSTRUMENT=instrument_name\n",
        "    start_time = time.time()\n",
        "    \n",
        "    COLAB_MODEL_DIR = COLAB_INSTRUMENTS_DIR + '/current'\n",
        "    print(f'Unziping {instrument_name}')\n",
        "    !rm -r $COLAB_MODEL_DIR\n",
        "    !unzip -j $COLAB_INSTRUMENTS_DIR'/'$instrument_name -d $COLAB_MODEL_DIR &> /dev/null\n",
        "  \n",
        "    #gin file\n",
        "    gin_file = os.path.join(COLAB_MODEL_DIR, 'operative_config-0.gin')\n",
        "    with gin.unlock_config():\n",
        "      gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "    #stats\n",
        "    dataset_stats_file = os.path.join(COLAB_MODEL_DIR, 'dataset_statistics.pkl')\n",
        "    print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "    try:\n",
        "      if tf.io.gfile.exists(dataset_stats_file):\n",
        "        with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "          DATASET_STATS = pickle.load(f)\n",
        "      else:\n",
        "          print('WARNING: pickle file not present')\n",
        "          DATASET_STATS = None\n",
        "    except Exception as err:\n",
        "      print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "  \n",
        "    #checkpoints\n",
        "    ckpt_files = [f for f in tf.io.gfile.listdir(COLAB_MODEL_DIR) if 'ckpt' in f]\n",
        "    ckpt_name = ckpt_files[0].split('.')[0]\n",
        "    ckpt = os.path.join(COLAB_MODEL_DIR, ckpt_name)\n",
        "    print('Restoring checkpoint %s took %.1f seconds' % (ckpt_name, time.time() - start_time))\n",
        "\n",
        "    if REALEXAMPLE != '' and REALINSTRUMENT != '':\n",
        "      print(\"Restoring model from load_instrument\")\n",
        "      restore_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSuV1jzkVCF1"
      },
      "source": [
        "def shift_loudness(audio_features, ld_shift=0.0):\n",
        "  #Shift loudness by a number of dB\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, octave_shift=0.0):\n",
        "  #Shift f0 by a number of octaves\n",
        "  audio_features['f0_hz'] *= 2.0 ** (octave_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y4Y3DGGgXMf"
      },
      "source": [
        "def restore_model():\n",
        "  global model, audio_features, ckpt\n",
        "\n",
        "  #Ensure dimensions and sampling rates are equal\n",
        "  time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "  n_samples_train = gin.query_parameter('Additive.n_samples')\n",
        "  hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "  time_steps = int(audio.shape[1] / hop_size)\n",
        "  n_samples = time_steps * hop_size\n",
        "\n",
        "  gin_params = [\n",
        "      'Additive.n_samples = {}'.format(n_samples),\n",
        "      'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "      'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "      'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
        "  ]\n",
        "\n",
        "  with gin.unlock_config():\n",
        "    gin.parse_config(gin_params)\n",
        "\n",
        "  # Trim all input vectors to correct lengths \n",
        "  for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "      audio_features[key] = audio_features[key][:time_steps]\n",
        "  audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "  #Set up the model just to predict audio given new conditioning\n",
        "  model = ddsp.training.models.Autoencoder()\n",
        "  model.restore(ckpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msHYBIy19FE7"
      },
      "source": [
        "#Set instrument and example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "LYJVJZFWjgIO"
      },
      "source": [
        "instrument_widget = widgets.Dropdown(options = instrument_file_list, value=instrument_file_list[2])\n",
        "interact(load_instrument, instrument_name=instrument_widget);\n",
        "\n",
        "example_widget = widgets.Dropdown(options = example_file_list, value=example_file_list[13])\n",
        "interact(load_example, example_name=example_widget);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7vulQQeiPcI"
      },
      "source": [
        "#Preprocess example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQw32hGA-g3"
      },
      "source": [
        "#Improve loudness with statistical info\n",
        "use_norm = True\n",
        "mask_threshold = 1\n",
        "quiet = 45\n",
        "\n",
        "#Improve pitch\n",
        "autotune_amount = 0\n",
        "\n",
        "#Adjust source material to model pitch and loudness\n",
        "octave_shift = 2\n",
        "loudness_shift = 0\n",
        "\n",
        "\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "mask_on, note_on_ratio = detect_notes(audio_features['loudness_db'],\n",
        "                                      audio_features['f0_confidence'],\n",
        "                                      mask_threshold, \n",
        "                                      exponent=2.0)\n",
        "\n",
        "if DATASET_STATS is not None:\n",
        "  _, loudness_norm = fit_quantile_transform(audio_features['loudness_db'],\n",
        "                                            mask_on,\n",
        "                                            inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "\n",
        "  mask_off = np.logical_not(mask_on)\n",
        "  loudness_norm[mask_off] -=  quiet * (1.0 - note_on_ratio[mask_off][:, np.newaxis])\n",
        "  loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape) \n",
        "  if use_norm:\n",
        "    audio_features_mod['loudness_db'] = loudness_norm \n",
        "\n",
        "\n",
        "if autotune_amount > 0:\n",
        "  f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "  tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "  f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune_amount, chromatic=True)\n",
        "  audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "\n",
        "audio_features_mod = shift_loudness(audio_features_mod, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, octave_shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqxUwVEKRDF6"
      },
      "source": [
        "#Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0lB108Y8PT8"
      },
      "source": [
        "print(\"EXAMPLE = '%s'\" % REALEXAMPLE)\n",
        "print(\"INSTRUMENT = '%s\" % REALINSTRUMENT)\n",
        "print('octave_shift = %.2f' % octave_shift)\n",
        "print('loudness_shift = %d' % loudness_shift)\n",
        "print('mask_threshold = %.2f' % mask_threshold)\n",
        "print('quiet = %d' % quiet)\n",
        "print('autotune_amount = %.2f' % autotune_amount)\n",
        "print()\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "\n",
        "outputs = model(audio_features_mod, training=False)\n",
        "audio_gen = model.get_audio_from_outputs(outputs)\n",
        "\n",
        "#Old code for DDSP v0.13.0\n",
        "#controls =  model.get_controls(audio_features_mod, training=False)\n",
        "#audio_gen = controls['processor_group']['signal']\n",
        "\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wCfTtaLsZRx"
      },
      "source": [
        "#auto octave adjustment.\n",
        "instrument_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "example_mean_pitch = np.mean(pitch[mask_on])\n",
        "\n",
        "#p_diff = instrument_mean_pitch - example_mean_pitch\n",
        "#p_diff_octave = p_diff / 12.0\n",
        "#round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "#p_diff_octave = round_fn(p_diff_octave)\n",
        "#audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(15.5, 10))\n",
        "ax[0].plot(audio_features['loudness_db'])\n",
        "ax[0].plot(loudness_norm)\n",
        "ax[0].legend(['db', 'norm'])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "ax[0].set_ylim(-100,0)\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz']))\n",
        "ax[1].plot(np.ones_like(mask_on) * instrument_mean_pitch, ':')\n",
        "ax[1].plot(np.ones_like(mask_on) * example_mean_pitch, ':')\n",
        "if autotune_amount > 0:\n",
        "  ax[1].plot(f0_midi_at)\n",
        "  ax[1].legend(['f0_hz', 'Instrument pitch','Example pitch', 'autotune'])\n",
        "else:\n",
        "  ax[1].legend(['f0_hz', 'Instrument pitch','Example pitch'])\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "ax[1].set_ylim(36,72)\n",
        "ax[2].plot(audio_features['f0_confidence'])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "ax[2].set_xlabel('Time step [frame]')\n",
        "ax[3].plot(np.ones_like(mask_on) * mask_threshold, 'k:')\n",
        "ax[3].plot(note_on_ratio)\n",
        "ax[3].plot(mask_on)\n",
        "ax[3].set_ylabel('Note-on Mask')\n",
        "ax[3].legend(['mask_threshold', 'note_on_ratio', 'mask_on'])\n",
        "_=ax[3].set_xlabel('Time step [frame]')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}